# BiomedCLIP VPT+TPT åˆ†å‰²æ¨¡å‹ - ä»£ç åˆ†æä¸ä¿®å¤æ€»ç»“

## âœ… ä»»åŠ¡å®Œæˆæƒ…å†µ

æ‚¨çš„ä»£ç  `/app/MixDSemi/SynFoCLIP/code/biomedclip_vpt_tpt_seg.py` **å·²æˆåŠŸå®Œæˆæ‰€æœ‰ä¸‰ä¸ªä»»åŠ¡**ï¼š

### 1. âœ… æ–‡æœ¬éƒ¨åˆ†åŠ å…¥ Learnable Token
- **å®ç°æ–¹å¼**: `TextPromptLearner` ç±»ç®¡ç†å¯å­¦ä¹ ä¸Šä¸‹æ–‡å‘é‡ (C_l)
- **ç»“æ„**: [CLS, **C_l**, Class, SEP] (CoOpé£æ ¼)
- **ç»´åº¦**: 4ä¸ªprompts Ã— 768ç»´ (BERTå†…éƒ¨ç»´åº¦)
- **ä½ç½®**: åœ¨ `encode_text_with_prompts()` æ–¹æ³•ä¸­ä¸ç±»åˆ«å­—ç¬¦ä¸²å…±åŒç¼–ç 

### 2. âœ… è§†è§‰ç¼–ç å™¨æ¯å±‚åŠ å…¥ Learnable Token  
- **å®ç°æ–¹å¼**: `VisualPromptLearner` ç±»ç®¡ç†è§†è§‰prompts (V_l)
- **ç»“æ„**: VPT-Deep æ¶æ„ï¼Œæ¯å±‚ Transformer æ³¨å…¥ç‹¬ç«‹çš„prompts
- **ç»´åº¦**: 12å±‚ Ã— 4ä¸ªprompts Ã— 768ç»´
- **ä½ç½®**: åœ¨ `_visual_forward_with_prompts()` æ–¹æ³•ä¸­é€å±‚æ³¨å…¥

### 3. âœ… æ”¯æŒ Tensor å›¾åƒè¾“å…¥
- **å®ç°æ–¹å¼**: `preprocess_tensor_images()` å‡½æ•°
- **åŠŸèƒ½**: 
  - è‡ªåŠ¨æ£€æµ‹è¾“å…¥èŒƒå›´ ([0,1], [-1,1], æˆ– [0,255])
  - è‡ªåŠ¨è°ƒæ•´å°ºå¯¸åˆ° 224Ã—224
  - åº”ç”¨ CLIP æ ‡å‡†åŒ– (mean/std)
  - æ”¯æŒå•é€šé“è‡ªåŠ¨æ‰©å±•åˆ°3é€šé“

---

## ğŸ› ä¸»è¦ä¿®å¤é—®é¢˜

### é—®é¢˜1: `text.proj` ä¸æ˜¯ Tensor è€Œæ˜¯ Sequential
**æŠ¥é”™**: `AttributeError: 'Sequential' object has no attribute 'shape'`

**åŸå› **: BiomedCLIP çš„ `text.proj` æ˜¯ä¸€ä¸ª Sequential å®¹å™¨:
```
Sequential(
  Linear(768 â†’ 640),
  GELU(),
  Linear(640 â†’ 512)
)
```

**ä¿®å¤**: ä½¿ç”¨ `model.text.output_dim` æˆ– `proj[-1].out_features` è·å–ç»´åº¦

### é—®é¢˜2: æ¨¡å‹å¯¹è±¡æ²¡æœ‰ `.device` å±æ€§
**æŠ¥é”™**: `AttributeError: 'CustomTextCLIP' object has no attribute 'device'`

**ä¿®å¤**: æ·»åŠ  `@property` æ–¹æ³•ä»å‚æ•°è·å–è®¾å¤‡:
```python
@property
def device(self) -> torch.device:
    return next(self.model.parameters()).device
```

### é—®é¢˜3: `pos_drop` å’Œ `patch_drop` æ˜¯æ¨¡å—æ–¹æ³•ï¼Œä¸æ˜¯ Tensor æ–¹æ³•
**æŠ¥é”™**: `AttributeError: 'Tensor' object has no attribute 'pos_drop'`

**ä¿®å¤**: æ”¹ä¸º `trunk.pos_drop(x)` è€Œé `x.pos_drop(x)`

### é—®é¢˜4: BiomedCLIP ä½¿ç”¨ BERT Tokenizerï¼Œæ²¡æœ‰ `sot_token_id`
**æŠ¥é”™**: `AttributeError: 'HFTokenizer' object has no attribute 'sot_token_id'`

**å…³é”®å‘ç°**: 
- BiomedCLIP ä½¿ç”¨ **HFTokenizer (BERTé£æ ¼)**
- ä½¿ç”¨ **CLS token (ID=2)** è€Œé SOT
- ä½¿ç”¨ **SEP token (ID=3)** è€Œé EOT
- å†…éƒ¨ç»´åº¦æ˜¯ **768** (BERT hidden size)ï¼Œè€Œé 512

**ä¿®å¤**: å®Œå…¨é‡å†™ `encode_text_with_prompts()`:
1. ä½¿ç”¨ `transformer.embeddings.word_embeddings` è·å–åµŒå…¥
2. æ„å»º [CLS, C_l, Class, SEP] åºåˆ—
3. æ·»åŠ  BERT çš„ position_embeddings å’Œ token_type_embeddings
4. é€šè¿‡ `transformer.encoder` å‰å‘ä¼ æ’­
5. ä½¿ç”¨ `pooler` æ± åŒ–ï¼ˆä¼ é€’å®Œæ•´çš„ encoder_outputsï¼‰
6. é€šè¿‡ `proj` æŠ•å½±åˆ° CLIP ç©ºé—´ (512ç»´)

### é—®é¢˜5: TextPrompt ç»´åº¦ä¸åŒ¹é…
**æŠ¥é”™**: `RuntimeError: Expected size 768 but got size 512`

**ä¿®å¤**: å°† `TextPromptConfig.embed_dim` ä» 512 æ”¹ä¸º 768

---

## ğŸ“Š æœ€ç»ˆè¿è¡Œç»“æœ

```
âœ“ VPT_TPT_CLIP_Seg (VPT-Deep + TPT-CoOp) åˆå§‹åŒ–æˆåŠŸ
  - è§†è§‰ Prompts (V_l, Deep): 36,864 å‚æ•°
  - æ–‡æœ¬ Prompts (C_l, CoOp): 3,072 å‚æ•°
  - æ€»å¯è®­ç»ƒå‚æ•°: 39,936 å‚æ•°

è¾“å‡º 'H_semantic_maps' (åˆ†å‰²å›¾) shape: torch.Size([2, 2, 196])
è¾“å‡º 'patch_features' (ç”¨äºä¸€è‡´æ€§) shape: torch.Size([2, 196, 512])
è®¡ç®—æ€»æŸå¤±: 3.9998

âœ“ è§†è§‰æç¤º (V_l) æ¢¯åº¦å·²è®¡ç®— (grad.norm = 2.56)
âœ“ æ–‡æœ¬æç¤º (C_l) æ¢¯åº¦å·²è®¡ç®— (grad.norm = 2.60)
âœ“ CLIP ä¸»å¹²å·²å†»ç»“ (æ— æ¢¯åº¦)
```

---

## ğŸ”‘ å…³é”®æŠ€æœ¯ç‚¹

### BiomedCLIP æ¶æ„ç‰¹ç‚¹
1. **è§†è§‰ç¼–ç å™¨**: ViT-B (Timm å®ç°)
   - 12å±‚ Transformer
   - 768 ç»´å†…éƒ¨ç‰¹å¾
   - è¾“å‡ºæŠ•å½±åˆ° 512 ç»´ CLIP ç©ºé—´

2. **æ–‡æœ¬ç¼–ç å™¨**: BERT (HuggingFace å®ç°)
   - 768 ç»´å†…éƒ¨ç‰¹å¾  
   - Sequential æŠ•å½±å±‚: 768â†’640â†’512
   - ä½¿ç”¨ CLS/SEP è€Œé SOT/EOT

### Prompt è®¾è®¡
- **VPT-Deep**: æ¯å±‚ç‹¬ç«‹çš„å¯å­¦ä¹  promptsï¼Œæ³¨å…¥åˆ° Transformer çš„ sequence æœ«å°¾
- **TPT-CoOp**: å•å±‚å¯å­¦ä¹ ä¸Šä¸‹æ–‡ï¼Œæ’å…¥åˆ° [CLS] å’Œ [Class] ä¹‹é—´
- **æ€»å‚æ•°**: ä»… ~40Kï¼Œç›¸æ¯”å®Œæ•´æ¨¡å‹ (~100M+) æè½»é‡

### è®­ç»ƒç­–ç•¥
- **å†»ç»“ä¸»å¹²**: ä»…è®­ç»ƒ promptsï¼Œå®ç° PEFT (Parameter-Efficient Fine-Tuning)
- **æ¢¯åº¦éªŒè¯**: ç¡®è®¤åªæœ‰ prompts æœ‰æ¢¯åº¦ï¼Œä¸»å¹²æ— æ¢¯åº¦
- **æƒé‡ä¿å­˜**: æ”¯æŒå•ç‹¬ä¿å­˜/åŠ è½½ prompts

---

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

```python
# 1. æ„å»ºæ¨¡å‹
from biomedclip_vpt_tpt_seg import build_vpt_tpt_seg_model

model, preprocess, tokenizer = build_vpt_tpt_seg_model(
    model_path="/root/models/BiomedCLIP",
    device="cuda",
    visual_num_prompts=4,
    text_num_prompts=4,
)

# 2. å‡†å¤‡è¾“å…¥
images = torch.rand(2, 3, 224, 224).cuda()  # æ”¯æŒ tensor è¾“å…¥
text_list = ["prostate", "background"]      # å­—ç¬¦ä¸²åˆ—è¡¨

# 3. å‰å‘ä¼ æ’­
outputs = model(images, text_list)
H_maps = outputs["H_semantic_maps"]  # [2, 2, 196] åˆ†å‰²å›¾
patches = outputs["patch_features"]   # [2, 196, 512] patchç‰¹å¾

# 4. ä¿å­˜ prompts
model.save_all_prompts("./my_prompts.pth")

# 5. åŠ è½½ prompts
model.load_all_prompts("./my_prompts.pth")
```

---

## âœ¨ ä»£ç ä¼˜åŠ¿

1. **å‚æ•°é«˜æ•ˆ**: ä»…è®­ç»ƒ 40K å‚æ•° (~0.04% çš„å®Œæ•´æ¨¡å‹)
2. **åŸŸä¸å˜æ€§**: VPT å­¦ä¹ åŸŸä¸å˜çš„è§†è§‰ç‰¹å¾ï¼ŒTPT å­¦ä¹ é²æ£’çš„è¯­ä¹‰åŸå‹
3. **å³æ’å³ç”¨**: å¯ä»¥è½»æ¾åˆ‡æ¢ä¸åŒçš„ prompts æƒé‡
4. **GPU å‹å¥½**: å°å‚æ•°é‡ï¼Œè®­ç»ƒé€Ÿåº¦å¿«ï¼Œæ˜¾å­˜å ç”¨ä½

---

## ğŸ“š å‚è€ƒ

- **VPT**: Visual Prompt Tuning (Deep variant)
- **CoOp**: Learning to Prompt for Vision-Language Models
- **BiomedCLIP**: A Multimodal Biomedical Foundation Model
- **PEFT**: Parameter-Efficient Fine-Tuning

æµ‹è¯•é€šè¿‡æ—¶é—´: 2025-11-04
